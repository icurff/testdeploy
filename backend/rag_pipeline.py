# ==== Imports ====
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.runnables import RunnableMap, RunnablePassthrough, RunnableLambda, RunnableConfig
from langchain_core.output_parsers import StrOutputParser

from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_community.chat_message_histories import DynamoDBChatMessageHistory
from langchain_tavily import TavilySearch

from elastic_search import bm25_search
from util import print_timestamp
# ==== Custom Retrieval Functions ====
from hyde import get_hypo_doc  # HyDE: sinh c√¢u h·ªèi gi·∫£ ƒë·ªãnh
from dense import find_similarity  # T√¨m ki·∫øm dense (vector)

from fusion import rrf_fusion  # Fusion hai k·∫øt qu·∫£




def retrieve(query: str, llm, embed_model, username: str) -> list[Document]:
    print_timestamp("üß† [HyDE] Generating hypothetical document...")
    hypo_doc = get_hypo_doc(query, llm)
    print("üìÑ Hypothetical Document:\n", hypo_doc)

    print_timestamp("üìç [Embed] Embedding hypo_doc and finding dense similarity...")
    hypo_emb = embed_model.embed_query(hypo_doc)
    dense_results = find_similarity(hypo_emb, k=10, embed_model=embed_model, username=username)
    print(f"üîé Dense results: {[doc.metadata.get('source', '') for doc in dense_results]}")

    print_timestamp("üîç Lexical search...")
    sparse_results = bm25_search(username,query)
    print(f"üßæ Sparse results: {[doc.metadata.get('source', '') for doc in sparse_results]}")

    print_timestamp("üîó [Fusion] RRF Fusion of dense + sparse...")
    combined_results = rrf_fusion([dense_results,sparse_results])
    print("‚úÖ Top 5 after fusion:")
    for i, doc in enumerate(combined_results[:5]):
        print(f"  {i + 1}.- {doc.page_content}...")

    return combined_results[:10]


# ==== Chains ====
def create_rag_chain(llm, embed_model, reranker, username):
    def _retrieve(inputs: dict) -> dict:
        query = inputs["question"]
        docs = retrieve(query, llm, embed_model, username)
        # print("‚öñÔ∏è [Reranker] Re-ranking fused results...")
        # reranked_docs = reranker.compress_documents(docs,query)

        return {
            "context": "\n\n".join(doc.page_content for doc in docs),
            "question": query,
            "history": inputs.get("history", [])
        }

    chain = RunnableLambda(_retrieve) | rag_prompt | llm | StrOutputParser()
    return RunnableWithMessageHistory(
        chain,
        get_session_history=get_history_session,
        input_messages_key="question",
        history_messages_key="history"
    )


search_tool = TavilySearch(max_results=3)


def create_search_chain(llm):
    def search_with_tavily(inputs: dict):
        query = inputs["question"]
        docs = search_tool.invoke(query)
        context = "\n\n".join(
            doc["content"] for doc in docs["results"] if doc.get("content")
        )
        return {
            "context": context,
            "question": query,
            "history": inputs.get("history", [])
        }

    chain = RunnableLambda(search_with_tavily) | search_prompt | llm | StrOutputParser()
    return RunnableWithMessageHistory(
        chain,
        get_session_history=get_history_session,
        input_messages_key="question",
        history_messages_key="history"
    )


def create_chat_chain(llm):
    chain = chat_prompt | llm | StrOutputParser()
    return RunnableWithMessageHistory(
        chain,
        get_session_history=get_history_session,
        input_messages_key="question",
        history_messages_key="history"
    )


# ==== Prompt Template ====
rag_prompt = ChatPromptTemplate.from_messages([
    (
        "system",
        """
B·∫°n l√† m·ªôt tr·ª£ l√Ω RAG ch√≠nh x√°c v√† s√∫c t√≠ch.
S·ª≠ d·ª•ng n·ªôi dung trong m·ª•c Context ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi. Quy t·∫Øc:
- N·∫øu th√¥ng tin kh√¥ng c√≥ trong Context, n√≥i r√µ: "T√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin trong t√†i li·ªáu." v√† ƒë·ªÅ xu·∫•t c√¢u h·ªèi l√†m r√µ ng·∫Øn g·ªçn.
- Tr√≠ch xu·∫•t ch√≠nh x√°c s·ªë li·ªáu, t√™n ri√™ng, ƒë·ªãnh nghƒ©a khi c√≥. Kh√¥ng b·ªãa.
- Tr·∫£ l·ªùi ng·∫Øn g·ªçn 2‚Äì5 c√¢u; d√πng g·∫°ch ƒë·∫ßu d√≤ng khi ph√π h·ª£p.
Context:
{context}
"""
    ),
    MessagesPlaceholder(variable_name="history", optional=True),
    ("human", "{question}"),
])

search_prompt = ChatPromptTemplate.from_messages([
    (
        "system",
        """
B·∫°n l√† m·ªôt tr·ª£ l√Ω t·ªïng h·ª£p t·ª´ k·∫øt qu·∫£ t√¨m ki·∫øm web.
K·∫øt h·ª£p c√°c ƒëo·∫°n li√™n quan trong Context ƒë·ªÉ tr·∫£ l·ªùi. Quy t·∫Øc:
- N√™u r√µ khi th√¥ng tin m√¢u thu·∫´n ho·∫∑c thi·∫øu.
- Tr√°nh suy ƒëo√°n; ch·ªâ d√πng n·ªôi dung trong Context.
- Cung c·∫•p c√¢u tr·∫£ l·ªùi ng·∫Øn g·ªçn v√† c√≥ c·∫•u tr√∫c.
Context (web results):
{context}
"""
    ),
    MessagesPlaceholder(variable_name="history", optional=True),
    ("human", "{question}"),
])

chat_prompt = ChatPromptTemplate.from_messages([
    (
        "system",
        """
B·∫°n l√† m·ªôt chatbot th√¢n thi·ªán, l·ªãch s·ª±, tr·∫£ l·ªùi ng·∫Øn g·ªçn, r√µ r√†ng.
∆Øu ti√™n tr·∫£ l·ªùi tr·ª±c ti·∫øp c√¢u h·ªèi; tr√°nh v√≤ng vo.
"""
    ),
    MessagesPlaceholder(variable_name="history", optional=True),
    ("human", "{question}"),
])


def get_history_session(session_id: str):
    # Gh√©p session_id t·ª´ user_id#conv_id ho·∫∑c t√°ch ra
    user_id, conv_id = session_id.split("#", 1)

    return DynamoDBChatMessageHistory(
        table_name="Conversations",
        session_id=session_id,
        primary_key_name="user_id",
        key={
            "user_id": user_id,
            "conv_id": conv_id,
        },
        history_messages_key="history"
    )
